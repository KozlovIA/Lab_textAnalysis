{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Вариант | Класс     |\n",
    "|---------|-----------|\n",
    "| 8       | 5, 16, 20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| № класса | Название класса          |\n",
    "|----------|--------------------------|\n",
    "| 5        | 'comp.sys.mac.hardware'  |\n",
    "| 16       | 'soc.religion.christian' |\n",
    "| 20       | 'talk.religion.misc'     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['comp.sys.mac.hardware', 'soc.religion.christian', 'talk.religion.misc'] \n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories, remove=remove)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Документ класса comp.sys.mac.hardware:\n",
      "A SIMM is a small PCB with DRAM chips soldered on.\n",
      "\n",
      "--maarten\n",
      "-------------------------------------------------\n",
      "Документ класса soc.religion.christian:\n",
      "\n",
      "[deletia- and so on]\n",
      "\n",
      "I seem to have been rather unclear.\n",
      "\n",
      "What I was asking is this:\n",
      "\n",
      "Please show me that the most effective substance-absure recovery\n",
      "programs involve meetinsg peoples' spiritual needs, rather than\n",
      "merely attempting to fill peoples' spiritual needs as percieved\n",
      "by the people, A.A, S.R.C. regulars, or snoopy. This will probably\n",
      "involve defining \"spritual needs\" (is it not that clear) and\n",
      "showing that such things exist and how they can be filled.\n",
      "\n",
      "Annother tack you might take is to say that \"fulfilling spiritual\n",
      "needs\" means \"acknowledging a \"higher power\" of some sort, then\n",
      "show that systems that do require this, work better than otherwise\n",
      "identical systems that do not. A correlation here would help you,\n",
      "but as you point out this might just be demonstrating swapping\n",
      "one crutch for annother. (however, I do feel that religion is\n",
      "usually a better crutch than alchohol, as it is not usually\n",
      "poisonous! :) )\n",
      "\n",
      "I hope with that clarification, my question will be answerable. I actually\n",
      "did know about the 12 step program, its the question of what it does,\n",
      "rather than what it tries to do, that makes a difference to me.\n",
      "---\n",
      "\t\t\t- Dan Johnson\n",
      "And God said \"Jeeze, this is dull\"... and it *WAS* dull. Genesis 0:0\n",
      "-------------------------------------------------\n",
      "Документ класса talk.religion.misc:\n",
      "RE: Red, wwhite, and black, the colors of the Imperial German war-flag --\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(categories)):\n",
    "    print(f'Документ класса {categories[i]}:')\n",
    "    print(twenty_train.data[np.where(twenty_train.target == i)[0][0]], \n",
    "          end='\\n-------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1554/1554 [00:06<00:00, 251.09it/s]\n",
      "100%|██████████| 1034/1034 [00:04<00:00, 232.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stem_text(text):\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    line = ' '.join([porter_stemmer.stem(word) for word in nltk_tokens])\n",
    "    return line\n",
    "\n",
    "stem_train = list(tqdm(map(stem_text, twenty_train.data), total=len(twenty_train.data)))\n",
    "stem_test = list(tqdm(map(stem_text, twenty_test.data), total=len(twenty_test.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frequency_words(train, test, class_num=None, stop_words=None):\n",
    "\n",
    "    if class_num is not None:\n",
    "        train = [twenty_train.data[i] for i in np.where(twenty_train.target == class_num)[0]]\n",
    "\n",
    "    vect = CountVectorizer(max_features = 10000, stop_words = stop_words)\n",
    "    train_data = vect.fit_transform(train)\n",
    "\n",
    "    x = list(zip(vect.get_feature_names_out(), np.ravel(train_data.sum(axis=0))))\n",
    "    x.sort(key=lambda row: row[1], reverse=True) \n",
    "    if class_num is not None:\n",
    "        print(f'Первые 20 наиболее частотных слов {class_num} класса:')\n",
    "    else:\n",
    "        print(f'Первые 20 наиболее частотных слов всей выборки:')\n",
    "    print(x[:20])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_jaccard_coef(first_list, second_list):\n",
    "    '''Рассчет коэффициента Жаккара'''\n",
    "    first_set = set(first_list)\n",
    "    second_set = set(second_list)\n",
    "    total = len(first_set.intersection(second_set))\n",
    "    coef = total / (len(first_set) + len(second_set) - total)\n",
    "    return coef\n",
    "\n",
    "def print_jaccard_coef(data, c0, c1, stop_words=None):\n",
    "    class0 = [data[i] for i in np.where(twenty_train.target == c0)[0]]\n",
    "    class1 = [data[i] for i in np.where(twenty_train.target == c1)[0]]\n",
    "    vect0 = CountVectorizer(max_features = 10000, stop_words = stop_words).fit(class0)\n",
    "    vect1 = CountVectorizer(max_features = 10000, stop_words = stop_words).fit(class1)\n",
    "    print(f'Коэффициент Жаккара между {c0} и {c1} классами:', end='\\t')\n",
    "    print(round(calc_jaccard_coef(vect0.get_feature_names_out(), vect1.get_feature_names_out()), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "дефолт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('the', 16652), ('to', 8490), ('of', 8334), ('and', 6656), ('that', 5747), ('is', 5591), ('in', 4801), ('it', 3830), ('you', 3092), ('not', 2749), ('for', 2699), ('this', 2486), ('be', 2316), ('are', 2220), ('have', 2166), ('as', 2136), ('with', 2071), ('on', 1823), ('but', 1818), ('was', 1622)]\n",
      "Первые 20 наиболее частотных слов 0 класса:\n",
      "[('the', 3290), ('to', 1544), ('and', 1248), ('of', 972), ('is', 966), ('it', 873), ('in', 748), ('for', 731), ('that', 706), ('with', 622), ('have', 534), ('on', 532), ('you', 531), ('this', 482), ('be', 410), ('if', 402), ('but', 361), ('or', 358), ('can', 357), ('not', 347)]\n",
      "Первые 20 наиболее частотных слов 1 класса:\n",
      "[('the', 8723), ('of', 4787), ('to', 4584), ('and', 3422), ('that', 3331), ('is', 3177), ('in', 2707), ('it', 1994), ('not', 1617), ('you', 1499), ('this', 1364), ('be', 1333), ('for', 1317), ('as', 1254), ('are', 1241), ('god', 1097), ('have', 1078), ('he', 1037), ('we', 1030), ('but', 1017)]\n",
      "Первые 20 наиболее частотных слов 2 класса:\n",
      "[('the', 4639), ('of', 2575), ('to', 2362), ('and', 1986), ('that', 1710), ('is', 1448), ('in', 1346), ('you', 1062), ('it', 963), ('not', 785), ('for', 651), ('as', 642), ('this', 640), ('are', 633), ('be', 573), ('have', 554), ('with', 508), ('was', 486), ('he', 470), ('they', 445)]\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.198\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.199\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.337\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=None)\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=0)\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=1)\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=2)\n",
    "\n",
    "print_jaccard_coef(twenty_train.data, 0, 1)\n",
    "print_jaccard_coef(twenty_train.data, 0, 2)\n",
    "print_jaccard_coef(twenty_train.data, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('god', 1427), ('people', 779), ('jesus', 722), ('know', 625), ('does', 624), ('just', 586), ('don', 571), ('think', 565), ('like', 559), ('say', 461), ('time', 444), ('believe', 437), ('good', 416), ('church', 414), ('bible', 411), ('christian', 396), ('way', 377), ('christ', 373), ('did', 373), ('christians', 333)]\n",
      "Первые 20 наиболее частотных слов 0 класса:\n",
      "[('mac', 327), ('apple', 266), ('drive', 211), ('use', 173), ('problem', 171), ('like', 163), ('know', 162), ('does', 160), ('bit', 150), ('just', 148), ('scsi', 142), ('don', 123), ('thanks', 120), ('card', 115), ('32', 113), ('memory', 112), ('new', 110), ('monitor', 106), ('disk', 105), ('ram', 103)]\n",
      "Первые 20 наиболее частотных слов 1 класса:\n",
      "[('god', 1097), ('jesus', 466), ('people', 452), ('church', 340), ('think', 337), ('does', 317), ('know', 314), ('believe', 298), ('don', 286), ('christ', 281), ('say', 280), ('just', 279), ('time', 266), ('like', 265), ('faith', 257), ('bible', 250), ('christians', 242), ('christian', 241), ('good', 211), ('did', 204)]\n",
      "Первые 20 наиболее частотных слов 2 класса:\n",
      "[('god', 329), ('people', 267), ('jesus', 256), ('don', 162), ('bible', 160), ('just', 159), ('christian', 151), ('think', 151), ('know', 149), ('say', 149), ('does', 147), ('did', 132), ('good', 131), ('like', 131), ('life', 118), ('way', 118), ('believe', 117), ('said', 103), ('point', 101), ('time', 99)]\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.182\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.183\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.321\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=None, stop_words='english')\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=0, stop_words='english')\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=1, stop_words='english')\n",
    "print_frequency_words(twenty_train.data, twenty_test.data, class_num=2, stop_words='english')\n",
    "\n",
    "print_jaccard_coef(twenty_train.data, 0, 1, stop_words='english')\n",
    "print_jaccard_coef(twenty_train.data, 0, 2, stop_words='english')\n",
    "print_jaccard_coef(twenty_train.data, 1, 2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "стоп-слова + стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('thi', 2494), ('wa', 1669), ('god', 1453), ('hi', 1004), ('christian', 909), ('ha', 867), ('doe', 788), ('peopl', 785), ('say', 759), ('use', 731), ('know', 720), ('jesu', 716), ('think', 659), ('ani', 658), ('onli', 625), ('like', 613), ('believ', 599), ('just', 586), ('time', 532), ('did', 518)]\n",
      "Первые 20 наиболее частотных слов 0 класса:\n",
      "[('mac', 327), ('apple', 266), ('drive', 211), ('use', 173), ('problem', 171), ('like', 163), ('know', 162), ('does', 160), ('bit', 150), ('just', 148), ('scsi', 142), ('don', 123), ('thanks', 120), ('card', 115), ('32', 113), ('memory', 112), ('new', 110), ('monitor', 106), ('disk', 105), ('ram', 103)]\n",
      "Первые 20 наиболее частотных слов 1 класса:\n",
      "[('god', 1097), ('jesus', 466), ('people', 452), ('church', 340), ('think', 337), ('does', 317), ('know', 314), ('believe', 298), ('don', 286), ('christ', 281), ('say', 280), ('just', 279), ('time', 266), ('like', 265), ('faith', 257), ('bible', 250), ('christians', 242), ('christian', 241), ('good', 211), ('did', 204)]\n",
      "Первые 20 наиболее частотных слов 2 класса:\n",
      "[('god', 329), ('people', 267), ('jesus', 256), ('don', 162), ('bible', 160), ('just', 159), ('christian', 151), ('think', 151), ('know', 149), ('say', 149), ('does', 147), ('did', 132), ('good', 131), ('like', 131), ('life', 118), ('way', 118), ('believe', 117), ('said', 103), ('point', 101), ('time', 99)]\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.199\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.207\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.341\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(stem_train, stem_test, class_num=None, stop_words='english')\n",
    "print_frequency_words(stem_train, stem_test, class_num=0, stop_words='english')\n",
    "print_frequency_words(stem_train, stem_test, class_num=1, stop_words='english')\n",
    "print_frequency_words(stem_train, stem_test, class_num=2, stop_words='english')\n",
    "\n",
    "print_jaccard_coef(stem_train, 0, 1, stop_words='english')\n",
    "print_jaccard_coef(stem_train, 0, 2, stop_words='english')\n",
    "print_jaccard_coef(stem_train, 1, 2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
    "train_data_tfidf = tfidf.fit_transform(stem_train)\n",
    "train_data_tfidf = tfidf.transform(stem_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def pipeline(x_train, y_train, x_test, stop_words=None, max_features=1000, use_idf=True):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(max_features=max_features, stop_words=stop_words)),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=use_idf)),\n",
    "                        ('clf', MultinomialNB()),], \n",
    "                        verbose=10)   \n",
    "    text_clf = text_clf.fit(x_train, y_train)\n",
    "    prediction = text_clf.predict(x_test)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def print_score(prediction, y_test):\n",
    "\n",
    "    precision = round(precision_score(prediction, y_test, average='weighted'), 3)\n",
    "    print ('Precision score: ', precision)\n",
    "    recall = round(recall_score(prediction, y_test, average='weighted'), 3)\n",
    "    print ('Recall score: ', recall)\n",
    "    f1 = round(f1_score(prediction, y_test, average='weighted'), 3)\n",
    "    print ('F1-score: ', f1)\n",
    "    accuracy = round(accuracy_score(prediction, y_test), 3)\n",
    "    print ('Accuracy score: ', accuracy)\n",
    "\n",
    "    return [precision, recall, f1, accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наличие / отсутствие стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.914\n",
      "Recall score:  0.752\n",
      "F1-score:  0.802\n",
      "Accuracy score:  0.752\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(twenty_train.data, twenty_train.target, twenty_test.data)\n",
    "results_withot_stem = print_score(prediction, twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.917\n",
      "Recall score:  0.766\n",
      "F1-score:  0.811\n",
      "Accuracy score:  0.766\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(stem_train, twenty_train.target, stem_test)\n",
    "results_stem = print_score(prediction, twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсечение \\ не отсечение стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.899\n",
      "Recall score:  0.786\n",
      "F1-score:  0.817\n",
      "Accuracy score:  0.786\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(stem_train, twenty_train.target, stem_test, stop_words='english')\n",
    "results_stop_words = print_score(prediction, twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взвешивание: Count, TF, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.914\n",
      "Recall score:  0.779\n",
      "F1-score:  0.817\n",
      "Accuracy score:  0.779\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', use_idf=False)\n",
    "results = print_score(prediction, twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество информативных терминов (max_features) - исследовать 5 значений в диапазоне от 100 до общего количества слов в выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.856\n",
      "Recall score:  0.682\n",
      "F1-score:  0.742\n",
      "Accuracy score:  0.682\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.899\n",
      "Recall score:  0.786\n",
      "F1-score:  0.817\n",
      "Accuracy score:  0.786\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.911\n",
      "Recall score:  0.783\n",
      "F1-score:  0.818\n",
      "Accuracy score:  0.783\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.941\n",
      "Recall score:  0.76\n",
      "F1-score:  0.816\n",
      "Accuracy score:  0.76\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.96\n",
      "Recall score:  0.74\n",
      "F1-score:  0.816\n",
      "Accuracy score:  0.74\n"
     ]
    }
   ],
   "source": [
    "max_featurs = [100, 1000, 2000, 5000, 10000]\n",
    "for i in max_featurs:\n",
    "    prediction = pipeline(stem_train, twenty_train.target, stem_test, stop_words='english', max_features=i)\n",
    "    results = print_score(prediction, twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
